{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNZsK_vkH5PQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to your audio files directory\n",
        "data_dir = '/kaggle/input/birdclef-2024/train_audio'\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv('/kaggle/input/birdclef-2024/train_metadata.csv')"
      ],
      "metadata": {
        "id": "QhEu0AOYIRLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from audio files\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
        "    mel_spect_scaled = np.mean(mel_spect_db.T, axis=0)\n",
        "    return mel_spect_scaled\n",
        "\n",
        "# Function to process a single row of the metadata\n",
        "def process_row(row):\n",
        "    file_path = os.path.join(data_dir, row['filename'])\n",
        "    if os.path.exists(file_path):\n",
        "        class_label = row['primary_label']\n",
        "        data = extract_features(file_path)\n",
        "        return data, class_label\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "G0KYGLqtITdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features and labels using parallel processing\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    futures = [executor.submit(process_row, row) for index, row in metadata.iterrows()]\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        data, class_label = future.result()\n",
        "        if data is not None:\n",
        "            features.append(data)\n",
        "            labels.append(class_label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_onehot = to_categorical(y_encoded)\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(le, 'label_encoder.pkl')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for CNN\n",
        "X_train = X_train.reshape((-1, 128, 1))\n",
        "X_test = X_test.reshape((-1, 128, 1))\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(128, 1, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 1)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(le.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "YJYL4AyBIfN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom callback to ensure minimum number of epochs\n",
        "class MinimumEpochs(Callback):\n",
        "    def __init__(self, min_epochs=30):\n",
        "        super(MinimumEpochs, self).__init__()\n",
        "        self.min_epochs = min_epochs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch + 1 < self.min_epochs:\n",
        "            self.model.stop_training = False\n",
        "\n",
        "# Callbacks\n",
        "min_epochs = 30\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "minimum_epochs = MinimumEpochs(min_epochs=min_epochs)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping, reduce_lr, minimum_epochs])\n",
        "\n",
        "# Save the model\n",
        "model.save('bird_cnn_model.h5')"
      ],
      "metadata": {
        "id": "1wxojqrqIrLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Multiply accuracy by 100 and print it\n",
        "accuracy_percent = accuracy * 100\n",
        "print(f'{accuracy_percent:.2f}%')"
      ],
      "metadata": {
        "id": "77bDw-f_IuAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if a file is an audio file\n",
        "def is_audio_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            return librosa.get_samplerate(file_path) is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Directory containing the test audio files\n",
        "test_dir = '../input/birdclef-2024/test_soundscapes/'\n",
        "if not os.path.exists(test_dir):\n",
        "    raise FileNotFoundError(f\"The directory {test_dir} does not exist.\")\n",
        "\n",
        "# List all files in the 'test_soundscapes' directory\n",
        "test_files = os.listdir(test_dir)\n",
        "\n",
        "# Filter out non-audio files\n",
        "test_files = [file for file in test_files if is_audio_file(os.path.join(test_dir, file))]\n",
        "\n",
        "# Function to extract features for test data\n",
        "def extract_features_test(file_path, file_name):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    time_window = 5.0\n",
        "    X_test = []\n",
        "    row_ids = []\n",
        "    for start_time in np.arange(0, duration, time_window):\n",
        "        end_time = start_time + time_window\n",
        "        if end_time > duration:\n",
        "            break\n",
        "        y_segment = y[int(start_time * sr):int(end_time * sr)]\n",
        "        mel_spect = librosa.feature.melspectrogram(y=y_segment, sr=sr, n_mels=128)\n",
        "        mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
        "        mel_spect_reshaped = mel_spect_db.T[:128, :1].reshape(128, 1, 1)\n",
        "        X_test.append(mel_spect_reshaped)\n",
        "        row_id = f\"{os.path.splitext(file_name)[0]}_{int(end_time)}\"\n",
        "        row_ids.append(row_id)\n",
        "    return X_test, row_ids\n",
        "\n",
        "X_test_list = []\n",
        "row_ids_list = []\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(extract_features_test, os.path.join(test_dir, file_name), file_name) for file_name in tqdm(test_files, desc=\"Submitting extraction tasks\")]\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting features\"):\n",
        "        X_test, row_ids = future.result()\n",
        "        X_test_list.extend(X_test)\n",
        "        row_ids_list.extend(row_ids)\n",
        "\n",
        "X_test = np.array(X_test_list)\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model('bird_cnn_model.h5')\n",
        "\n",
        "# Perform predictions\n",
        "batch_size = 32\n",
        "predictions = []\n",
        "for i in tqdm(range(0, len(X_test), batch_size), desc=\"Making predictions\"):\n",
        "    batch_predictions = model.predict(X_test[i:i + batch_size])\n",
        "    predictions.extend(batch_predictions)\n",
        "\n",
        "# Load the label encoder\n",
        "le = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Create a submission DataFrame\n",
        "submission_df = pd.DataFrame(predictions, columns=[f'bird_id_{i}' for i in range(1, len(le.classes_) + 1)])\n",
        "submission_df.insert(0, 'row_id', row_ids_list)\n",
        "\n",
        "# Map bird IDs to species names\n",
        "bird_id_to_species = {f'bird_id_{i+1}': species for i, species in enumerate(le.classes_)}\n",
        "\n",
        "# Rename columns in the submission DataFrame\n",
        "submission_df = submission_df.rename(columns=bird_id_to_species)\n",
        "\n",
        "# Save the submission file\n",
        "submission_path = 'submission.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(\"Updated submission file 'submission.csv' has been created successfully.\")"
      ],
      "metadata": {
        "id": "MEtfB6GtIy36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}